# CUPAC Variance Reduction Experiments

Содержит реализацию и сравнение классического метода CUPED и расширенного ML-подхода CUPAC на синтетических и реальных данных.

## Содержание репозитория
- `autocupac.py` — реализация AutoCUPAC трансформера
- `data_gen.py` — генерация синтетических данных для тестов методов
- `runner.ipynb` — минимальный пример использования AutoCUPAC на синтетических данных
- `R&D synthetic data.ipynb` — первичные эксперименты и проверка идеи на синтетике
- `RealData1_CUPAC.ipynb` — применение AutoCUPAC на первом реальном датасете
- `RealData2_CUPAC.ipynb` — повторное подтверждение результатов на другом реальном датасете
- `WaldEstimator/` — вспомогательные исследования (оценка LATE / Wald) вне основной линии CUPAC, сохранено для полноты R&D

## Ноутбуки и результаты
| Ноутбук | Назначение | Ключевой вывод |
|---------|------------|----------------|
| `runner.ipynb` | Быстрый старт (синтетика) | Демонстрация базового использования AutoCUPAC |
| `R&D synthetic data.ipynb` | Синтетические сценарии, чувствительность | Демонстрация стабильного выигрыша CUPAC над базовой метрикой |
| `RealData1_CUPAC.ipynb` | Реальные данные (набор 1) | снижение дисперсии 34% |
| `RealData2_CUPAC.ipynb` | Реальные данные (набор 2) | снижение дисперсии 33% |

## Методы
**CUPED** (Controlled Experiments Using Pre-Experiment Data) — линейная корректировка целевой метрики с использованием заранее известной ковариаты (pre-period метрики). 

$$
\tilde{Y} = Y - \theta (X - \mathbb{E}[X]), \qquad \theta = \frac{\text{Cov}(Y,X)}{\text{Var}(X)}
$$

После корректировки дисперсия снижается до

$$
\text{Var}(\tilde{Y}) = (1-\rho_{Y,X}^2)\text{Var}(Y),
$$
где $\rho_{Y,X}$ — коэффициент корреляции. Максимальное относительное снижение дисперсии равно $\rho_{Y,X}^2$.

**CUPAC** (обобщение) — ML-подход, строящий оптимальную линейную комбинацию множества ковариат (лаги, агрегаты, производные признаки) через регрессию/модель предсказания. Пусть $X \in \mathbb R^p$ — вектор pre-treatment признаков. Тогда линейный оптимум:

$$
\tilde{Y} = Y - \beta^\top (X - \mathbb{E}[X]), \qquad \beta^{*} = \Sigma_{XX}^{-1} \Sigma_{XY}
$$

и
$$
\text{Var}(\tilde{Y}) = \text{Var}(Y) - \Sigma_{YX}\Sigma_{XX}^{-1}\Sigma_{XY} = (1-R^2)\text{Var}(Y),
$$
где $R^2$ — коэффициент детерминации регрессии $Y$ на $X$. Следовательно Var.Red.% $= 100\cdot R^2$ (при корректной оценке и отсутствии переобучения). В ML-варианте вместо явной формулы для $\beta$ берётся прогноз $\hat Y = f(X)$ из выбранной модели, и используется остаток $R = Y - \hat Y$.

CUPAC:
- Поддержка множества признаков (а не одной ковариаты)
- Перебор/валидация нескольких моделей (Linear, Ridge, Lasso, CatBoost)
- Автовыбор модели по максимальному снижению дисперсии
- Отчет по важности признаков и достигнутому проценту сокращения дисперсии